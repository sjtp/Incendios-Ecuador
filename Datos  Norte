---
title: "Regresión Logistica zona norte"
author: "Sandra Torres, Xavier Zapata-Ríos"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    self_contained: no
    keep_md: yes
  word_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R - cargar librerías

Contenido

*   Plots variables
*   Algoritmo Boruta
*   Preprosesamiento
*   Modelamiento 
*   Validación

## R - cargar librerías
```{r ,include = T,eval="FALSE",message = FALSE,warning = FALSE ,results = "hide" }
# Borrar memoria
# https://es.r4ds.hadley.nz/r-markdown.html
rm(list=ls())
gc()
Sys.setenv(tz="UTC")
packages<-c("recipes","dplyr","caret","car","PerformanceAnalytics",
            "ggplot2","lattice","ggpubr","lubridate","tidyr",
            "corrplot","purrr","randomForest","tibble","data.table",
            "ranger","pROC","sm","aod","survey",
            "MASS", #stepAIC 
            "RColorBrewer","ggcorrplot",
            #Nestor
            "openxlsx","magrittr","readr","hexbin","GGally",
            "modelr","lmtest","sandwich","boot","leaps",
            "glmnet","broom","sigr","caTools","FSelector",
            # ST
            "revealjs", #RMarkdown
            "gridExtra","grid",
            "mgcv", #libro
            "knitr",
            "Boruta",#https://www.listendata.com/2017/05/feature-selection-boruta-package.html
            "kableExtra",#Para mostrar mejor las tablas#https://ggplot2-book.org/index.html
            "olsrr",#VIF lm
            "regclass" ,   #VIF
            "epiDisplay", #logistic.display OD
#Raster plor
            "sp","writexl","raster","rgdal","tidyverse","sf","ggsn","fda","ggrepel",
            "rasterVis","scales","grid","scales","viridis","ggthemes","ggnewscale","metR",
            "cowplot",  # get_legend() & plot_grid() functions
            "patchwork", # blank plot: plot_spacer()
#The Hosmer-Lemeshow test is a statistical test for goodness of fit for logistic regression models.
            "ResourceSelection", #hoslem.test ,Hosmer-Lemeshow Goodness of Fit (GOF) Test 
            "LOGIT", #Display Hosmer-Lemeshow statistic and table of probabilities 
# following logistic regression using glm with binomial family
            "DescTools"
)
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
ipak(packages)
```
## Importar tabla resumen datos

```{r }
library(readxl)
Variables0 <- read_excel("2_Datos/Variables.xlsx")
Variables <-  Variables0[ , -which(names(Variables0) %in% c("Variables","Type", "Resolution"))]
```

## Variables

```{r, echo=FALSE}
Variables %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "Cambria")
```


## Importar data

```{r }
# Leer el archivo csv
datos0 <- read.table("2_Datos/Tabla_final_norte.csv",header = T,sep=";",
dec = ".")
# datos0 <- read_csv("2_Datos/Tabla_final_norte.csv", sep=";")

datos1 <-  datos0[ , -which(names(datos0) %in% c("FID", "MES","ANIO","X_m","Y_m","cus_n1", "ZONA"))]
str(datos1)
```

## Explorar la data
```{r }
summary(datos1)
```

## Explorar la data

```{r ,include = T,message = FALSE,warning = FALSE }
levels(datos1$CUS)
#Cambiar el formato de los datos y reorganizar
datos1 <- datos1 %>%
  dplyr::mutate(
    Class = factor(Class,levels = c(0,1)),
    CUS = factor(CUS,levels = c("VEGETACION ARBUSTIVA Y HERBACEA","BOSQUE","TIERRA AGROPECUARIA")),
   Aspect_1= factor(Aspect_1, levels = c("F","N","NE","NW","S","SE","SW","W","E"))
)
# datos1 <- datos1 %>% mutate(datos1, Aspect_1 = factor(ifelse(Aspect_1 %in% c("F","N","S"), "Asp_F_N_S",
#                                      ifelse(Aspect_1 %in% c("NE","SE"), "Asp_NE_SE", "Asp_NW_SW"))))
# datos1 <- datos1 %>% mutate(datos1, Aspect_1 = factor(ifelse(Aspect_1 %in% c("F"), "Asp_F",
#                                      ifelse(Aspect_1 %in% c("SW","SE"), "Asp_SW_SE", 
#                                      ifelse(Aspect_1 %in% c("NW","SW"), "Asp_NW_SW", 
#                                      ifelse(Aspect_1 %in% c("N"), "Asp_N", "Asp_S"))))))
levels(datos1$CUS)
levels(datos1$Aspect_1)
levels(datos1$CUS) <- c("ARB","BOS", "AGRO")
levels(datos1$Class) <- c("No", "Yes") #do not have "0"'s and "1"s, Modelamiento
str(datos1)

```

## Including Plots

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
plot_colors <- c("gray50", "orangered2")
p1 <- ggplot(datos1, aes(x=temp_mm, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p2 <- ggplot(datos1, aes(x=temp_mm, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p3 <- ggplot(data = datos1, aes(x = Class, y = temp_mm, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  scale_color_manual(values = plot_colors) +
  theme_bw()+
  geom_jitter(alpha = 0.3, width = 0.15) 
p4 <- ggplot(datos1, aes(x=temp_mjj, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p5 <- ggplot(datos1, aes(x=temp_mjj, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
  scale_fill_manual(values = plot_colors) +
  theme_bw()
p6 <- ggplot(data = datos1, aes(x = Class, y = temp_mjj, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()

grid.arrange(p1, p2, p3, p4,p5,p6 ,ncol=3)

```

## Including Plots

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
p1 <- ggplot(datos1, aes(x=prec_mjj, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p2 <- ggplot(datos1, aes(x=prec_mjj, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p3 <- ggplot(data = datos1, aes(x = Class, y = prec_mjj, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  scale_color_manual(values = plot_colors) +
  theme_bw()+
  geom_jitter(alpha = 0.3, width = 0.15) 
p4 <- ggplot(datos1, aes(x=preci_mm, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p5 <- ggplot(datos1, aes(x=preci_mm, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p6 <- ggplot(data = datos1, aes(x = Class, y = preci_mm, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()
p7 <- ggplot(datos1, aes(x=preci_ma, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p8 <- ggplot(datos1, aes(x=preci_ma, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p9 <- ggplot(data = datos1, aes(x = Class, y = preci_ma, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()

grid.arrange(p1, p2, p3, p4,p5,p6 ,p7,p8,p9,ncol=3)

```
## Including Plots

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
p1 <- ggplot(datos1, aes(x=ndvi_tot, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p2 <- ggplot(datos1, aes(x=ndvi_tot, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p3 <- ggplot(data = datos1, aes(x = Class, y = ndvi_tot, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  scale_color_manual(values = plot_colors) +
  theme_bw()+
  geom_jitter(alpha = 0.3, width = 0.15) 
p4 <- ggplot(datos1, aes(x=ndvi_mjj, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p5 <- ggplot(datos1, aes(x=ndvi_mjj, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p6 <- ggplot(data = datos1, aes(x = Class, y = ndvi_mjj, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()

grid.arrange(p1, p2, p3, p4,p5,p6,ncol=3)

```
## Including Plots

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
p1 <- ggplot(datos1, aes(x=ndvi_tot, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p2 <- ggplot(datos1, aes(x=ndvi_tot, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p3 <- ggplot(data = datos1, aes(x = Class, y = ndvi_tot, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  scale_color_manual(values = plot_colors) +
  theme_bw()+
  geom_jitter(alpha = 0.3, width = 0.15) 
p4 <- ggplot(datos1, aes(x=ndvi_mjj, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p5 <- ggplot(datos1, aes(x=ndvi_mjj, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p6 <- ggplot(data = datos1, aes(x = Class, y = ndvi_mjj, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()
p7 <- ggplot(datos1, aes(x=vci_tot, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p8 <- ggplot(datos1, aes(x=vci_tot, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p9 <- ggplot(data = datos1, aes(x = Class, y = vci_tot, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()

grid.arrange(p1, p2, p3, p4,p5,p6 ,p7,p8,p9,ncol=3)

```

## Including Plots

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
p1 <- ggplot(datos1, aes(x=road_m, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p2 <- ggplot(datos1, aes(x=road_m, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p3 <- ggplot(data = datos1, aes(x = Class, y = road_m, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  scale_color_manual(values = plot_colors) +
  theme_bw()+
  geom_jitter(alpha = 0.3, width = 0.15) 
p4 <- ggplot(datos1, aes(x=urban_m, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p5 <- ggplot(datos1, aes(x=urban_m, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p6 <- ggplot(data = datos1, aes(x = Class, y = urban_m, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()
p7 <- ggplot(datos1, aes(x=dem_m, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p8 <- ggplot(datos1, aes(x=dem_m, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p9 <- ggplot(data = datos1, aes(x = Class, y = dem_m, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()

grid.arrange(p1, p2, p3, p4,p5,p6 ,p7,p8,p9,ncol=3)

```

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
p1 <- ggplot(datos1, aes(x=CUS, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p2 <- ggplot(datos1, aes(x=road_m, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p3 <- ggplot(data = datos1, aes(x = Class, y = road_m, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  scale_color_manual(values = plot_colors) +
  theme_bw()+
  geom_jitter(alpha = 0.3, width = 0.15) 
p4 <- ggplot(datos1, aes(x=urban_m, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p5 <- ggplot(datos1, aes(x=urban_m, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p6 <- ggplot(data = datos1, aes(x = Class, y = urban_m, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()
p7 <- ggplot(datos1, aes(x=dem_m, y=Class)) + geom_point(alpha=0.33) + geom_smooth()+
  scale_color_manual(values = plot_colors) +
  theme_bw()
p8 <- ggplot(datos1, aes(x=dem_m, group=Class, color=Class, fill=Class)) + geom_density(alpha=0.4)+
   scale_color_manual(values = plot_colors) + scale_fill_manual(values = plot_colors) +
  theme_bw()
p9 <- ggplot(data = datos1, aes(x = Class, y = dem_m, color = Class)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = plot_colors) +
  theme_bw()

grid.arrange(p1, p2, p3, p4,p5,p6 ,p7,p8,p9,ncol=3)

```

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
p1 <- ggplot(datos1, aes(x=CUS, fill=Class)) + stat_count(width = 0.5)+
  scale_fill_manual(values = plot_colors) + scale_color_manual(values = plot_colors) +
  theme_bw()

p2 <- ggplot(datos1, aes(x=Aspect_1, group=Class, color=Class, fill=Class))  + stat_count(width = 0.5)+
     scale_fill_manual(values = plot_colors) + scale_color_manual(values = plot_colors) +
     theme_bw()
grid.arrange(p1, p2,ncol=2)

```


## Including Plots

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
# Sacar un gráfico de correlación entre las variables continuas 
# names(datos1)
datos_cor <-  datos1[ , -which(names(datos1) %in% c("Class", "Aspect_1","CUS"))]%>% cor()

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

plot1 <- ggcorrplot(datos_cor, method = c("square"),type = c("lower"), 
           ggtheme = ggplot2::theme_classic(base_size = 11, base_line_size= 0.2), title = "",
           show.legend = TRUE, legend.title = "Corr", show.diag = FALSE,
           colors = c("#A90B09", "white", "#2889ce"), 
           outline.color = "#F5F1F1", 
           hc.order = TRUE, hc.method = "complete", lab = TRUE,
           lab_col = "black", lab_size = 3, sig.level = 0.01,
           insig = c("blank"), 
           tl.cex = 11, tl.col = "black", tl.srt = 90,
           digits = 2)+
  theme(axis.title.x = element_text(angle=0,hjust=-0.17,vjust=16,size=11))+
  labs(x = "Variables")
plot1
```

## PCA computation
```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}
mtcars.pca <- prcomp(datos1[,-c(1,13,14)], center = TRUE,scale. = TRUE)

```  

```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}

library(ggbiplot)
mtcars.pca %>% 
  ggbiplot::ggbiplot( scale = 1, 
                      groups = datos1$Class, 
                      ellipse = T) +
  geom_vline(xintercept = 0, linetype = 3)+
  geom_hline(yintercept = 0, linetype = 3)+
  ggpubr::theme_pubclean() + 
  theme(legend.position = "top", legend.title = element_blank(), legend.key = element_blank())+
  theme(
      # plot.title = element_text(hjust = 0.5),#hjust 0.5 centrado
          text = element_text(size=16),#texto tamaño letra
          strip.text = element_blank(),#Remove facet_wrap labels completely letrero TRUE en grafico
            panel.background = element_rect(fill = "white"),#fondo grafico
          strip.background = element_blank(),#Remove facet_wrap labels completely
          axis.line = element_line(colour = "gray40", size = rel(1)), #linea ejes
          axis.ticks=element_line(size = 0.8, color="black"),##
          axis.ticks.length = unit(-0.25,"cm"),
          axis.text.y.right= element_text(margin=unit(c(0,0,0,3), "mm")),
          axis.text.y=element_text(margin=unit(c(0,6,0,0), "mm"),hjust = -0.8),# , ,
          axis.text.x = element_text(angle = 0, hjust = 0.8, vjust = -1.5),#posición texto eje x
          panel.grid = element_line(colour = "grey88"),#color grilla
          # plot.margin = margin(0.2,0.6,0.8,.6, "cm"),#Superior, Izquierda, 
          panel.border = element_rect(linetype = "solid", fill = NA,colour = "gray40"),#borde completo plot
          plot.subtitle = element_text(color = "black"),
          plot.caption = element_text(hjust = 0.5,size = 14),
      legend.text = element_text(size = 16),
      legend.position = "bottom",
      legend.key.height = unit(0.8, "cm"),
      legend.key.width  = unit(1.5, "cm"))
```


## Prioritization and determination of effective factors using Boruta algorithm
```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}
boruta0 <- Boruta(Class~., data= datos1, doTrace=2, maxRuns=500)
plot(boruta0, las= 2, cex.axis=0.7)

```

## Considering variables importance using by Boruta algorithm

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}

## Asignar importancia a las variables
weight <- chi.squared( Class~., data= datos1)
df_weight <- data.frame(var= rownames(weight), weight)
df_weight[order(-weight), ]
```


## Considering variables importance using by Boruta algorithm

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}
by_mediamImp <- attStats(boruta0) 
by_mediamImp <- as.data.frame(by_mediamImp)
by_mediamImp <- data.frame(rownames(by_mediamImp), 
                           round(by_mediamImp$meanImp,2),
                           round(by_mediamImp$medianImp,2),
                           round(by_mediamImp$minImp,2),
                           round(by_mediamImp$maxImp,2),
                           by_mediamImp$decision
                           )
colnames(by_mediamImp) <- c("Factors", "Mean_importance", "Median importante", "Min importance",
                             "Max importance","Decision")

Variables <- by_mediamImp %>% arrange(desc(Mean_importance))
write.csv(Variables, "Variables_boruta.csv")
Variables %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria") 

```


## Prioritization and determination of effective factors using Boruta
algorithm
```{r ,include = T,message = FALSE,warning = FALSE,echo=FALSE}
plotImpHistory(boruta0, las= 2, cex.axis=0.7)
```


## Seleccionar variables a utilizar
```{r ,include = T,message = FALSE,warning = FALSE}
## Asignar importancia a las variables
# datos <-  datos1[ , -which(names(datos1) %in% c("temp_mm", "preci_mm","preci_ma","ndvi_mjj","dem_m"))]
datos <-  datos1[ , -which(names(datos1) %in% c("dem_m", "preci_mm","preci_ma","ndvi_mjj","vci_tot","temp_mjj"))]
str(datos)
```


## Including Plots
```{r,include = T,message = FALSE,warning = FALSE, echo=FALSE}
# Sacar un gráfico de correlación entre las variables continuas 
datos_cor <-  datos[ , -which(names(datos) %in% c("Class", "Aspect_1","CUS"))]%>%cor()
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
plot1 <- ggcorrplot(datos_cor, method = c("square"),type = c("lower"), 
           ggtheme = ggplot2::theme_classic(base_size = 11, base_line_size= 0.2), title = "",
           show.legend = TRUE, legend.title = "Corr", show.diag = FALSE,
           colors = c("#A90B09", "white", "#2889ce"), 
           outline.color = "#F5F1F1", 
           hc.order = TRUE, hc.method = "complete", lab = TRUE,
           lab_col = "black", lab_size = 3, sig.level = 0.01,
           insig = c("blank"), 
           tl.cex = 11, tl.col = "black", tl.srt = 90,
           digits = 2)+
  theme(axis.title.x = element_text(angle=0,hjust=-0.17,vjust=16,size=11))+
  labs(x = "Variables")
plot1
```

## DIVISIÓN DE LOS DATOS EN ENTRENAMIENTO Y TEST
Se empieza particionando el dataset, esta vez será una partición 80-20

```{r ,include = T,message = FALSE,warning = FALSE }
set.seed(123)
train <- createDataPartition(y = datos$Class, p = 0.8, list = FALSE, times = 1)
#Asignar los datos a variables de entrenamiento y test
datos_train <- datos[train, ]
datos_test  <- datos[-train, ]
```


## DIVISIóN DE LOS DATOS EN ENTRENAMIENTO Y TEST
```{r ,include = T,message = FALSE,warning = FALSE}
#comprobar que tienen la misma distribuciÃ³n de los incendios totales
prop.table(table(datos_train$Class)) %>% round(digits = 2)
prop.table(table(datos_test$Class))  %>% round(digits = 2)
```

## PREPROCESAMIENTO DE LOS DATOS
(Machine Learning con R y caret)[https://www.cienciadedatos.net/documentos/41_machine_learning_con_r_y_caret#Selecci%C3%B3n_de_predictores]

```{r ,include = T,message = FALSE,warning = FALSE}

#4. PREPROCESAMIENTO DE LOS DATOS
#Ploteo de las variables
names(datos)
# plot(datos_train1, pch = as.numeric(datos$Class),#objeto no encontrado datos_train1
#      col = as.numeric(datos$Class))
#Crear una receta con los datos para el modelo
objeto_recipe1 <- recipe(formula = Class~prec_mjj+temp_mm+ndvi_tot+CUS+ slope_perc+
                           urban_m+road_m+river_m+Aspect_1,datos)
#Se estandarizan todas las variables numericas
objeto_recipe1 <- objeto_recipe1 %>% step_center(all_numeric())
objeto_recipe1 <- objeto_recipe1 %>% step_scale(all_numeric())
objeto_recipe1 <- objeto_recipe1 %>% step_BoxCox(all_numeric())

objeto_recipe1 <- objeto_recipe1 %>% step_dummy(all_nominal(), -all_outcomes())



```

## PREPROCESAMIENTO DE LOS DATOS
```{r ,include = T,message = FALSE,warning = FALSE}
# Se entrena el objeto recipe
trained_recipe1 <- prep(objeto_recipe1, training = datos_train)
trained_recipe1
summary(trained_recipe1)
# str(trained_recipe1)
```

## PREPROCESAMIENTO DE LOS DATOS
```{r ,include = T,message = FALSE,warning = FALSE}
# Se aplican las transformaciones al conjunto de entrenamiento y de test
datos_train_prep1 <- bake(trained_recipe1, new_data = datos_train)
datos_test_prep1  <- bake(trained_recipe1, new_data = datos_test)
glimpse(datos_train_prep1)
datos_total_prep1 <- bake(trained_recipe1, new_data = datos)
```



## Control
```{r ,include = T,message = FALSE,warning = FALSE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 10
repeticiones <- 5

# Hiperparámetros
hiperparametros <- data.frame(parameter = "none")

set.seed(123)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

myControl <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE,
                            classProbs = TRUE,   savePredictions = TRUE,
                            summaryFunction = twoClassSummary
                          )

```

## Preprocesamiento y modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}

df_train_mod <- datos_train_prep1 #%>% dplyr::select(!c(river_m, Aspect_1))
df_test_mod <- datos_test_prep1 #%>% dplyr::select(!c(river_m, Aspect_1))

# str(df_train_mod)
set.seed(123)
model_1 <- train(
  Class~., 
  df_train_mod,
  method = "glm",
  family= "binomial",
  trControl = myControl
)
```
## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
model_1_ <- glm(Class~., df_train_mod, family = binomial(link="logit"))
```


## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
summary(model_1$finalModel)
```

## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
model_1
```

## Seleccionar variables a utilizar
```{r ,include = T,message = FALSE,warning = FALSE}
varImp(model_1)
```
## PREPROCESAMIENTO DE LOS DATOS

Con el data set sin dummys
```{r ,include = T,message = FALSE,warning = FALSE}

step.model <- model_1_ %>% stepAIC(trace = FALSE)
coef(step.model)
```
## Modelamiento

Entrenamos quitando variables
```{r ,include = T,message = FALSE,warning = FALSE}
modelo_logistic_AIC <- train(
  # Class~prec_mjj+temp_mm+ndvi_mjj+CUS_BOS+CUS_AGRO+ slope_perc+urban_m+road_m,
  Class~prec_mjj+temp_mm+ndvi_tot+slope_perc+urban_m+road_m+CUS_BOS+CUS_AGRO+ Aspect_1_NE+ Aspect_1_NW,                  
  data = df_train_mod,
                         method = "glm",
                                family = "binomial")
modelo_logistic_AIC

```
## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
summary(modelo_logistic_AIC)
```
## Modelamiento

Entrenamos quitando variables
```{r ,include = T,message = FALSE,warning = FALSE}
modelo_logistic_AIC2 <- train(
  # Class~prec_mjj+temp_mm+ndvi_mjj+CUS_BOS+CUS_AGRO+ slope_perc+urban_m+road_m,
  Class~prec_mjj+temp_mm+ndvi_tot+slope_perc+urban_m+CUS_BOS+CUS_AGRO+ Aspect_1_NE,                  
  data = df_train_mod,
                         method = "glm",
                          family = "binomial")
modelo_logistic_AIC

```

## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
summary(modelo_logistic_AIC2)
```
## Modelamiento

Entrenamos quitando variables
```{r ,include = T,message = FALSE,warning = FALSE}
# df_train_dummy$Class <- datos_train$Class
# set.seed(123)
# model_2 <- train(
#    # Class~urban_m+slope_perc+preci_mm+temp_mm+ndvi_mjj+CUS, #Carmen
#   # Class~prec_mjj+temp_mjj+ndvi_tot+CUS.ARB+ vci_tot+slope_perc+road_m+urban_m,
#    Class~prec_mjj+temp_mm+ndvi_mjj+CUS_BOS+CUS_AGRO+ slope_perc+urban_m+road_m,
#   # "prec_mjj"   "temp_mm"    "ndvi_mjj"   "CUS.ARB"    "slope_perc" "urban_m"    "road_m"
#   df_train_mod,
#   # preProcess = preProc,
#   method = "glm",
#   family= "binomial",
#   trControl = myControl
# )
# # HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 10
repeticiones <- 5

# Hiperparámetros
hiperparametros <- data.frame(parameter = "none")

set.seed(123)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_logistic <- train(   Class~prec_mjj+temp_mm+ndvi_tot+slope_perc+urban_m+CUS_BOS+CUS_AGRO+ Aspect_1_NE,                  
  data = df_train_mod,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic

```

## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
summary(modelo_logistic)
```
## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
varImp(modelo_logistic)
```

## Apply model to train_df
```{r ,include = T,message = FALSE,warning = FALSE}

predictions <- predict(modelo_logistic, newdata = datos_train_prep1)
confusionMatrix(data = predictions, datos_train_prep1$Class)
```
## Apply model to test_df
```{r ,include = T,message = FALSE,warning = FALSE}

predictions <- predict(modelo_logistic, newdata = datos_test_prep1)
confusionMatrix(data = predictions, datos_test_prep1$Class)
```

## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
# Hacer modelo de regresion logistica
m1 <- glm( Class~prec_mjj+temp_mm+ndvi_tot+slope_perc+urban_m+CUS_BOS+CUS_AGRO+ Aspect_1_NE,                  
  data = df_train_mod, family = binomial(link="logit"))
# Vemos el modelo
summary(m1)
```

## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
m1
```
## Modelamiento
```{r ,include = T,message = FALSE,warning = FALSE}
logistic.display(m1)
exp(cbind(coef(m1), confint(m1)))
```

## Modelamiento, medidas de bondad
```{r ,include = T,message = FALSE,warning = FALSE}
# Hacer modelo de regresion logistica
anova(m1,test="Chisq")
```

## Coefficients and Wald test for logistic regresion

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}
 # Class~prec_mjj+temp_mm+ndvi_tot+slope_perc+urban_m+CUS_BOS+CUS_AGRO+ Aspect_1_NE,
 #  data = df_train_mod

tw_preci_mjj <- regTermTest(modelo_logistic$finalModel, "prec_mjj")
tw_temp_mm <- regTermTest(modelo_logistic$finalModel, "temp_mm")
tw_ndvi_tot <- regTermTest(modelo_logistic$finalModel, "ndvi_tot")

# tw_ndvi_mjj <- regTermTest(modelo_logistic$finalModel, "ndvi_mjj")
tw_CUS_BOS <- regTermTest(modelo_logistic$finalModel, "CUS_BOS")
tw_CUS_AGRO <- regTermTest(modelo_logistic$finalModel, "CUS_AGRO")
tw_slope_perc <- regTermTest(modelo_logistic$finalModel, "slope_perc")
# tw_road_m <- regTermTest(modelo_logistic$finalModel, "road_m")
tw_urban <- regTermTest(modelo_logistic$finalModel, "urban_m")
# tw_Aspect_1_NW <- regTermTest(modelo_logistic$finalModel, "Aspect_1_NW")
tw_Aspect_1_NE <- regTermTest(modelo_logistic$finalModel, "Aspect_1_NE")
# tw_Aspect_1_E  <- regTermTest(modelo_logistic$finalModel, "Aspect_1_E")

Wald <- round(c(NA,tw_preci_mjj$Ftest, tw_temp_mm$Ftest,tw_ndvi_tot$Ftest, tw_slope_perc$Ftest, 
                tw_urban$Ftest,tw_CUS_BOS$Ftest, tw_CUS_AGRO$Ftest,tw_Aspect_1_NE$Ftest),2)

df1 <- round(c(NA,tw_preci_mjj$df, tw_temp_mm$df,tw_ndvi_tot$df,  tw_slope_perc$df, 
               tw_urban$df,tw_CUS_BOS$df, tw_CUS_AGRO$df,tw_Aspect_1_NE$df),2)

tabla <- round(as.data.frame(modelo_logistic$finalModel$coefficients),3)
colnames(tabla) <- c("Coefficient")
SE <- round(summary(m1)$coefficients[, 2],3)
VIF <- round(c(NA, vif(m1)),2)
CI  <- round(confint.default(m1),2)
z_value <- round(summary(m1)$coefficients[, 3],3)
P <- round(summary(m1)$coefficients[, 4],3)
OR <-round((exp(cbind(coef(m1), confint(m1)))),2)
# exp(coef(m1)) #ORs
# names(summary(m1))

Tablafinal <- data.frame(tabla,SE, Wald,df1, VIF, z_value, P, CI, OR)
colnames(Tablafinal) <- c("Coefficient", "SE","Wald", "df","VIF","z_value","P","Lower","Upper","OD","Lower","Upper")
write.csv2(Tablafinal,"Tablafinal.csv")
Tablafinal %>%kbl() %>%kable_classic(full_width = F, html_font = "Cambria") %>% 
  add_footnote("SE, standard error; df, degrees of freedom; VIF, variance inflation factor; CI, confidence interval; OR, odds radio", notation="none") %>% 
  add_header_above(c(" " = 1," " = 7, "95% CI for Coefficients" = 2, " " = 1, "95% CI for OD" = 2))

```


## Predicted Values from Logistic Regression
pag 212 Peter Bruce etal, 2020, Practical Statistics for Data Scientists_ 50+ Essential Concepts Using R and Python
```{r ,include = T,message = FALSE,warning = FALSE}
m_total<- m1
saveRDS(m_total, "m_total.RDS")
```

## Statistical test for goodness of fit
Next we pass the outcome y and model fitted probabilities to the hoslem.test function, choosing g=10 groups:

```{r ,include = T,message = FALSE,warning = FALSE}
hl <- ResourceSelection::hoslem.test(m1$y, fitted(m1), g=10)
hl
```
## Statistical test for goodness of fit
pag 89, Hilbe_J_2015_Practical Guide to Logistic Regression
Display Hosmer-Lemeshow statistic and table of probabilities following logistic regression using glm with binomial family
Y1 observed event fire, Y1hat associated predicted event fire
```{r ,include = T,message = FALSE,warning = FALSE}
HLChi10 <- HLTest(obj = m1,g= 10)
cbind(HLChi10$observed, round(HLChi10$expect, digits = 1))

```
## Statistical test for goodness of fit
pag 89, Hilbe_J_2015_Practical Guide to Logistic Regression
Display Hosmer-Lemeshow statistic and table of probabilities following logistic regression using glm with binomial family
```{r ,include = T,message = FALSE,warning = FALSE}
HLChi10
```
## Statistical test for goodness of fit
pag 89, Hilbe_J_2015_Practical Guide to Logistic Regression
Display Hosmer-Lemeshow statistic and table of probabilities following logistic regression using glm with binomial family
Y1 observed event fire, Y1hat associated predicted event fire
```{r ,include = T,message = FALSE,warning = FALSE}
HLChi12 <- HLTest(obj = m1,g= 12)
HLChi12
cbind(HLChi12$observed, round(HLChi12$expect, digits = 1))
```



## Statistical test for goodness of fit
pag 89, Hilbe_J_2015_Practical Guide to Logistic Regression
Display Hosmer-Lemeshow statistic and table of probabilities following logistic regression using glm with binomial family
```{r ,include = T,message = FALSE,warning = FALSE}
HLChi12
```


## Statistical test for goodness of fit
Display Hosmer-Lemeshow statistic and table of probabilities following logistic regression using glm with binomial family
```{r ,include = T,message = FALSE,warning = FALSE}

LOGIT::hlGOF.test(m1$y, predict(m1, df_train_mod,
                                 type='response'), breaks=10)
```


## Predicted Values from Logistic Regression
pag 212 Peter Bruce etal, 2020, Practical Statistics for Data Scientists_ 50+ Essential Concepts Using R and Python
```{r ,include = T,message = FALSE,warning = FALSE}
pred <- predict(m1)
summary(pred)
prob <- 1/(1 + exp(-pred))
summary(prob)


```
## Predicted Values from Logistic Regression
pag 217 Peter Bruce etal, 2020, Practical Statistics for Data Scientists_ 50+ Essential Concepts Using R and Python

```{r ,include = T,message = FALSE,warning = FALSE}

logistic_gam <- gam(Class~s(prec_mjj)+temp_mm+ndvi_tot+slope_perc+urban_m+CUS_BOS+CUS_AGRO+ Aspect_1_NE,                  
  data = df_train_mod, family='binomial')
```

## Analysis of residuals
pag 217 Peter Bruce etal, 2020, Practical Statistics for Data Scientists_ 50+ Essential Concepts Using R and Python

```{r ,include = T,message = FALSE,warning = FALSE}
terms <- predict(logistic_gam, type='terms')
partial_resid <- resid(m1) + terms
df <- data.frame(prec_mjj = datos_train_prep1[, 'prec_mjj'],
terms = terms[, 's(prec_mjj)'],
partial_resid = partial_resid[, 's(prec_mjj)'])
ggplot(df, aes(x=prec_mjj, y=partial_resid, solid = FALSE)) +
geom_point(shape=46, alpha=0.4) +
geom_line(aes(x=prec_mjj, y=terms),
color='red', alpha=0.5, size=1.5) +
labs(y='Partial Residual')

```

## Evaluating Classification Models
pag 220 Peter Bruce etal, 2020, Practical Statistics for Data Scientists_ 50+ Essential Concepts Using R and Python

Accuracy: The percent (or proportion) of cases classified correctly.

Confusion matrix : A tabular display (2×2 in the binary case) of the record counts by their predicted
and actual classification status.

Sensitivity: The percent (or proportion) of all 1s that are correctly classified as 1s.Synonym, Recall

Specificity: The percent (or proportion) of all 0s that are correctly classified as 0s.

Precision: The percent (proportion) of predicted 1s that are actually 1s.

ROC curve: A plot of sensitivity versus specificity.

Lift: A measure of how effective the model is at identifying (comparatively rare) 1s at
different probability cutoffs.


## Evaluating Classification Models

$$accuracy= \frac {\sum_{}^{} TruePositive+ \sum_{}^{} TrueNegative}{SampleSize}$$
$$accuracy= \frac {ΣTruePositive}{\sum_{}^{} TruePositive+ \sum_{}^{} FalsePositive}$$
$$accuracy= \frac {ΣTruePositive}{\sum_{}^{} TruePositive+ \sum_{}^{} FalseNegative}$$

## Evaluating Classification Models

```{r ,include = T,message = FALSE,warning = FALSE}
pred <- predict(m1, newdata=datos_train_prep1)
pred_y <- as.numeric(pred > 0)
true_y <- as.numeric(df_train_mod$Class=='Yes')
true_pos <- (true_y==1) & (pred_y==1)
true_neg <- (true_y==0) & (pred_y==0)
false_pos <- (true_y==0) & (pred_y==1)
false_neg <- (true_y==1) & (pred_y==0)
conf_mat <- matrix(c(sum(true_pos), sum(false_pos),
sum(false_neg), sum(true_neg)), 2, 2)
colnames(conf_mat) <- c('Yhat = 1', 'Yhat = 0')
rownames(conf_mat) <- c('Y = 1', 'Y = 0')
conf_mat
```
## Evaluating Classification Models

```{r ,include = T,message = FALSE,warning = FALSE}

# precision
conf_mat[1, 1] / sum(conf_mat[,1])
# recall
conf_mat[1, 1] / sum(conf_mat[1,])
# specificity
conf_mat[2, 2] / sum(conf_mat[2,])
```

## ROC Curve

```{r ,include = T,message = FALSE,warning = FALSE}
idx <- order(-pred)
recall <- cumsum(true_y[idx] == 1) / sum(true_y == 1)
specificity <- (sum(true_y == 0) - cumsum(true_y[idx] == 0)) / sum(true_y == 0)
roc_df <- data.frame(recall = recall, specificity = specificity)
ggplot(roc_df, aes(x=specificity, y=recall)) +
geom_line(color='blue') +
scale_x_reverse(expand=c(0, 0)) +
scale_y_continuous(expand=c(0, 0)) +
geom_line(data=data.frame(x=(0:100) / 100), aes(x=x, y=1-x),
linetype='dotted', color='red')
```

## AUC, 
pag63, Amiri,2019
Generally, if the values of AUC are: 

*   0.9–1, excellent
*   0.8–0.9, very good
*   0.7–0.8, good 
*   0.6–0.7 moderate
*   0.5–0.6 poor

(Yesilnacar, 2005)

```{r ,include = T,message = FALSE,warning = FALSE}
sum(roc_df$recall[-1] * diff(1 - roc_df$specificity))
```

## Lift
pag. 227 
```{r ,include = T,message = FALSE,warning = FALSE}
sum(roc_df$recall[-1] * diff(1 - roc_df$specificity))
```


## Modelamiento
Con el comando drop1 revisamos si el AIC se ve afectado al eliminar una variable
```{r ,include = T,message = FALSE,warning = FALSE}
drop_1 <- drop1(m1, test = 'LRT')
drop_1
# drop1(m1, test = 'Chisq')

```



## Bondad de ajuste

pseudo-R2, mientras más cercano a 1, mejor
```{r ,include = T,message = FALSE,warning = FALSE}

# Metodo 1
broom::glance(modelo_logistic$finalModel) %>%
  summarize(pR2 = 1 - deviance/null.deviance)
# Metodo 2
wrapChiSqTest(modelo_logistic$finalModel)
```


## Bondad de ajuste
pseudo-R2, mientras más cercano a 1, mejor
```{r ,include = T,message = FALSE,warning = FALSE}


PseudoR2 <- DescTools::PseudoR2(m1, c("McFadden", "McFaddenAdj","CoxSnell","Nagelkerke","AldrichNelson","VeallZimmermann"
              ,"McKelveyZavoina","Efron","Tjur","AIC","LogLik","LogLikNull","G2"))

PseudoR2_f <- as.data.frame(PseudoR2)
PseudoR2_f
write.csv(PseudoR2_f, "Pseudonorte.csv")
```


## Bondad de ajuste
pseudo-R2, mientras más cercano a 1, mejor
```{r ,include = T,message = FALSE,warning = FALSE}
#Modelo con todos los datos
predicciones_raw2 <- predict(modelo_logistic, newdata = datos_total_prep1,
                             type = "prob")
glimpse(predicciones_raw2)
predicciones_final <- data.frame(datos,datos_total_prep1,predicciones_raw2,datos0$X_m,datos0$Y_m)
names(predicciones_final)
str(predicciones_final)
# predicciones_final <- predicciones_final %>% dplyr::select(1:22,30,31)

# write.csv(predicciones_final,file = "predicciones_final.csv" )
```

## Modelamiento

```{r ,include = T,message = FALSE,warning = FALSE}
# Hacer modelo de regresion logistica
varImp(m1)
```

## Modelamiento

```{r ,include = T,message = FALSE,warning = FALSE}
# Hacer modelo de regresion logistica
modelo_logistic2 <- train(Class~prec_mjj+temp_mm,
                         data = df_train_mod,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic2
```
## Modelamiento

```{r ,include = T,message = FALSE,warning = FALSE}
# Hacer modelo de regresion logistica
modelo_logistic3 <- train(Class~prec_mjj+temp_mm+ndvi_tot,
                         data = df_train_mod,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic3
```

## Modelamiento

```{r ,include = T,message = FALSE,warning = FALSE}
modelo_logistic4 <- train(Class~prec_mjj+temp_mm+ndvi_tot+slope_perc,
                         data = df_train_mod,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic4
```

## Modelamiento

```{r ,include = T,message = FALSE,warning = FALSE}
modelo_logistic5 <- train(Class~prec_mjj+temp_mm+ndvi_tot+slope_perc+urban_m,
                         data = df_train_mod,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic5
```


## Modelamiento


```{r ,include = T,message = FALSE,warning = FALSE}
modelo_logistic6 <- train(Class~prec_mjj+temp_mm+ndvi_tot+slope_perc+urban_m+CUS_BOS,
                         data = df_train_mod,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic6
```

```{r ,include = T,message = FALSE,warning = FALSE}
modelo_logistic7 <- train(Class~prec_mjj+temp_mm+ndvi_tot+slope_perc+urban_m+CUS_BOS+CUS_AGRO,
                         data = df_train_mod,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic7

```


## Tabla

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}
modelo_logistic_2 <- round(summary(modelo_logistic2)$coefficients[, 2],3)
modelo_logistic_2
modelo_logistic_3 <- round(summary(modelo_logistic3)$coefficients[, 2],3)
modelo_logistic_3
modelo_logistic_4 <- round(summary(modelo_logistic4)$coefficients[, 2],3)
modelo_logistic_4
modelo_logistic_5 <- round(summary(modelo_logistic5)$coefficients[, 2],3)
modelo_logistic_5
modelo_logistic_6 <- round(summary(modelo_logistic6)$coefficients[, 2],3)
modelo_logistic_6
modelo_logistic_7 <- round(summary(modelo_logistic7)$coefficients[, 2],3)
modelo_logistic_7

```

## Matriz de confusion

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}

# Agregar predicciones en probabilidad
test_pred <- df_test_mod %>% 
  dplyr::mutate(
    PREDICCION_PROB= predict(model_1, newdata= df_test_mod, type="prob")$Yes  ##Note la minuscula, depende del nivel del factor
  )
colnames(test_pred)
# Definir umbral de 0.5
test_pred %<>% 
  dplyr::mutate(
    PREDICCION= factor(as.integer(PREDICCION_PROB > 0.5), levels=c(1,0), labels=c("Yes","No"))
  )

with(test_pred, table(PREDICCION, Class))
```

## Matriz de confusion

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}

# Definir umbral de 0.5
test_pred %<>% 
  mutate(
    PREDICCION= factor(as.integer(PREDICCION_PROB > 0.2), levels=c(1,0), labels=c("Si","No"))
  )

with(test_pred, table(PREDICCION, Class))
```

## Matriz de confusion

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}

optimizacion <- as_tibble( data.frame(umbral= seq(0.05, 0.95, 0.05), ganancia= NA))
getGanancia <- function(test_pred, u){
  test_pred %<>% 
    mutate(
      PREDICCION= factor(as.integer(PREDICCION_PROB > u), levels=c(1,0), labels=c("Si","No"))
    )
  matriz_conf <- with(test_pred, table(PREDICCION, Class))
  100 * matriz_conf[1,1] - 50 * matriz_conf[1,2]
}

getGanancia(test_pred, 0.2)


optimizacion %<>% group_by(umbral) %>% mutate(ganancia= getGanancia(test_pred, umbral))
optimizacion
```
## Matriz de confusion

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}

optimizacion %>% arrange(desc(ganancia))
```

## Matriz de confusion

```{r ,include = T,message = FALSE,warning = FALSE, echo=FALSE}

plot(optimizacion)  

```


## Mapa final
<center>![](Probability_of_fire_events_Norte.png){width=850px}</center>


## Gracias
<!-- <center>![](4Rio.jpg){width=750px}</center> -->
Contacto
torres.sandra.p.j@gmail.com





 
 
